{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sistema Jurídico RAG Avançado (Notebook Demo)\n",
        "\n",
        "Este notebook apresenta uma implementação de referência baseada na arquitetura de 4 camadas descrita na documentação técnica. Ele foca em:\n",
        "- Extração estruturada (Docling)\n",
        "- Indexação baseada em raciocínio (PageIndex)\n",
        "- Gestão de contexto conversacional (ChatIndex)\n",
        "- Persistência auditável (Google Drive)\n",
        "\n",
        "Inclui também um chat jurídico robusto com rastreabilidade e logs de auditoria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependências e configuração\n",
        "\n",
        "Este notebook foi estruturado para rodar localmente ou em ambientes como Colab. Ajuste os caminhos e chaves conforme necessário."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional\n",
        "import asyncio\n",
        "import requests\n",
        "from docling.document_converter import DocumentConverter\n",
        "from playwright.async_api import async_playwright\n",
        "import hashlib\n",
        "import json\n",
        "\n",
        "# Configuração base (ajuste conforme seu ambiente)\n",
        "@dataclass(frozen=True)\n",
        "class ConfigSistema:\n",
        "    pageindex_api_url: str = \"https://api.pageindex.ai/v1/index\"\n",
        "    pageindex_api_key: str = \"SUA_CHAVE_AQUI\"\n",
        "    chatindex_dir: Path = Path(\"./chatindex\")\n",
        "    drive_root: Path = Path(\"./Juridico_Unificado\")\n",
        "    audit_dir: Path = Path(\"./Juridico_Unificado/05_Auditoria\")\n",
        "    cache_dir: Path = Path(\"./cache_juridico\")\n",
        "\n",
        "config = ConfigSistema()\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Camada 1: Orquestração\n",
        "\n",
        "A classe `SistemaJuridicoUnificado` atua como facade, coordenando extração, chunking, indexação, scraping e auditoria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Integrações reais (Docling, PageIndex, scraping)\n",
        "\n",
        "Os componentes abaixo conectam o fluxo a extração real via Docling, indexação na API do PageIndex e scraping com Playwright, incluindo rate limiting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SistemaExtracaoDocling:\n",
        "    def __init__(self) -> None:\n",
        "        self.converter = DocumentConverter()\n",
        "\n",
        "    def extrair(self, documento_path: Path) -> Dict[str, Any]:\n",
        "        resultado = self.converter.convert(str(documento_path))\n",
        "        documento = resultado.document\n",
        "        texto = documento.export_to_text()\n",
        "        return {\n",
        "            \"documento_id\": documento_path.stem,\n",
        "            \"texto_completo\": texto,\n",
        "            \"metadata\": getattr(resultado, \"metadata\", {}),\n",
        "        }\n",
        "\n",
        "\n",
        "class PageIndexClient:\n",
        "    def __init__(self, api_url: str, api_key: str) -> None:\n",
        "        self.api_url = api_url\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def indexar(self, documento_id: str, chunks: List[str]) -> Dict[str, Any]:\n",
        "        resposta = requests.post(\n",
        "            self.api_url,\n",
        "            json={\"documento_id\": documento_id, \"chunks\": chunks},\n",
        "            headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n",
        "            timeout=30,\n",
        "        )\n",
        "        resposta.raise_for_status()\n",
        "        return resposta.json()\n",
        "\n",
        "\n",
        "class SistemaScrapingJuridico:\n",
        "    def __init__(self, rate_limit_seconds: float = 1.0) -> None:\n",
        "        self.rate_limit_seconds = rate_limit_seconds\n",
        "\n",
        "    async def coletar_texto(self, url: str) -> str:\n",
        "        async with async_playwright() as playwright:\n",
        "            browser = await playwright.chromium.launch()\n",
        "            page = await browser.new_page()\n",
        "            await page.goto(url, wait_until=\"networkidle\")\n",
        "            await asyncio.sleep(self.rate_limit_seconds)\n",
        "            texto = await page.inner_text(\"body\")\n",
        "            await browser.close()\n",
        "            return texto\n",
        "\n",
        "\n",
        "class ChatIndexPersistente:\n",
        "    def __init__(self, storage_path: Path) -> None:\n",
        "        self.storage_path = storage_path\n",
        "        self.storage_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def registrar(self, conversa: Dict[str, Any]) -> None:\n",
        "        with self.storage_path.open(\"a\", encoding=\"utf-8\") as handle:\n",
        "            handle.write(json.dumps(conversa, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    def carregar(self) -> List[Dict[str, Any]]:\n",
        "        if not self.storage_path.exists():\n",
        "            return []\n",
        "        return [\n",
        "            json.loads(linha)\n",
        "            for linha in self.storage_path.read_text(encoding=\"utf-8\").splitlines()\n",
        "            if linha.strip()\n",
        "        ]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SistemaAuditoriaUnificado:\n",
        "    def __init__(self, audit_dir: Path):\n",
        "        self.audit_dir = audit_dir\n",
        "        self.audit_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.log_central: List[Dict[str, Any]] = []\n",
        "        self.hash_registry: Dict[str, Dict[str, str]] = {}\n",
        "\n",
        "    def registrar_evento(self, categoria: str, evento: Dict[str, Any]) -> str:\n",
        "        evento_id = f\"evt_{hashlib.md5(str(evento).encode()).hexdigest()[:10]}\"\n",
        "        evento[\"timestamp\"] = evento.get(\"timestamp\", datetime.now().isoformat())\n",
        "\n",
        "        if self.log_central:\n",
        "            evento[\"hash_anterior\"] = self.hash_registry[self.log_central[-1][\"evento_id\"]][\"hash\"]\n",
        "\n",
        "        hash_atual = hashlib.md5(json.dumps(evento, sort_keys=True).encode()).hexdigest()\n",
        "        self.hash_registry[evento_id] = {\"hash\": hash_atual, \"timestamp\": evento[\"timestamp\"]}\n",
        "\n",
        "        registro = {**evento, \"evento_id\": evento_id, \"hash_atual\": hash_atual}\n",
        "        self.log_central.append(registro)\n",
        "        self._persistir_log(categoria, registro)\n",
        "        return evento_id\n",
        "\n",
        "    def _persistir_log(self, categoria: str, registro: Dict[str, Any]) -> None:\n",
        "        destino = self.audit_dir / f\"{categoria}.jsonl\"\n",
        "        with destino.open(\"a\", encoding=\"utf-8\") as handle:\n",
        "            handle.write(json.dumps(registro, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "class SistemaJuridicoUnificado:\n",
        "    def __init__(self, config: ConfigSistema):\n",
        "        self.config = config\n",
        "        self.auditoria = SistemaAuditoriaUnificado(config.audit_dir)\n",
        "        self.extrator = SistemaExtracaoDocling()\n",
        "        self.pageindex = PageIndexClient(config.pageindex_api_url, config.pageindex_api_key)\n",
        "        self.scraper = SistemaScrapingJuridico()\n",
        "        self.chatindex = ChatIndexPersistente(config.chatindex_dir / \"historico.jsonl\")\n",
        "\n",
        "    def _chunking_semantico(self, texto: str, tamanho: int = 800) -> List[str]:\n",
        "        return [texto[i:i + tamanho] for i in range(0, len(texto), tamanho)]\n",
        "\n",
        "    def processar_documento(self, documento_path: Path) -> Dict[str, Any]:\n",
        "        self.auditoria.registrar_evento(\"processamento\", {\n",
        "            \"tipo\": \"inicio_processamento\",\n",
        "            \"documento\": documento_path.name,\n",
        "        })\n",
        "        extracao = self.extrator.extrair(documento_path)\n",
        "        chunks = self._chunking_semantico(extracao[\"texto_completo\"])\n",
        "        indexacao = self.pageindex.indexar(extracao[\"documento_id\"], chunks)\n",
        "        arvore = {\"documento_id\": documento_path.stem, \"estrutura_arvore\": {\"raiz\": {\"titulo\": documento_path.stem}}}\n",
        "\n",
        "        self.auditoria.registrar_evento(\"processamento\", {\n",
        "            \"tipo\": \"fim_processamento\",\n",
        "            \"documento\": documento_path.name,\n",
        "            \"chunks_gerados\": len(chunks),\n",
        "        })\n",
        "        return {\"extracao\": extracao, \"chunks\": chunks, \"arvore\": arvore, \"indexacao\": indexacao}\n",
        "\n",
        "    def responder_consulta(self, consulta: str, contexto: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        self.auditoria.registrar_evento(\"consultas\", {\n",
        "            \"tipo\": \"consulta\",\n",
        "            \"consulta\": consulta,\n",
        "        })\n",
        "        resposta = {\n",
        "            \"consulta\": consulta,\n",
        "            \"resposta\": f\"Resumo jurídico para: {consulta}\",\n",
        "            \"fontes\": contexto.get(\"fontes\", []),\n",
        "        }\n",
        "        self.auditoria.registrar_evento(\"consultas\", {\n",
        "            \"tipo\": \"resposta\",\n",
        "            \"consulta\": consulta,\n",
        "            \"hash_resposta\": hashlib.md5(json.dumps(resposta, ensure_ascii=False).encode()).hexdigest(),\n",
        "        })\n",
        "        return resposta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Camada 2: Busca híbrida e ranking\n",
        "\n",
        "A classe `SistemaBuscaHibrida` combina sinais do PageIndex e de embeddings vetoriais, com pesos configuráveis e explicabilidade do ranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SistemaBuscaHibrida:\n",
        "    def __init__(self, peso_pageindex: float = 0.6, peso_vetorial: float = 0.4):\n",
        "        self.peso_pageindex = peso_pageindex\n",
        "        self.peso_vetorial = peso_vetorial\n",
        "\n",
        "    def _normalizar(self, resultados: list[dict[str, float]]) -> dict[str, float]:\n",
        "        if not resultados:\n",
        "            return {}\n",
        "        max_score = max(item.get(\"score\", 0.0) for item in resultados) or 1.0\n",
        "        return {item[\"id\"]: item.get(\"score\", 0.0) / max_score for item in resultados}\n",
        "\n",
        "    def rankear(self, resultados_pageindex: list[dict[str, float]], resultados_vetoriais: list[dict[str, float]]):\n",
        "        pageindex_norm = self._normalizar(resultados_pageindex)\n",
        "        vetorial_norm = self._normalizar(resultados_vetoriais)\n",
        "        ids = set(pageindex_norm) | set(vetorial_norm)\n",
        "        combinados = []\n",
        "\n",
        "        for doc_id in ids:\n",
        "            score_pageindex = pageindex_norm.get(doc_id, 0.0)\n",
        "            score_vetorial = vetorial_norm.get(doc_id, 0.0)\n",
        "            score_final = (\n",
        "                self.peso_pageindex * score_pageindex +\n",
        "                self.peso_vetorial * score_vetorial\n",
        "            )\n",
        "            combinados.append({\n",
        "                \"id\": doc_id,\n",
        "                \"score_final\": score_final,\n",
        "                \"score_pageindex\": score_pageindex,\n",
        "                \"score_vetorial\": score_vetorial,\n",
        "            })\n",
        "\n",
        "        combinados.sort(key=lambda item: item[\"score_final\"], reverse=True)\n",
        "        return {\n",
        "            \"resultados\": combinados,\n",
        "            \"explicabilidade\": {\n",
        "                \"criterios\": (\n",
        "                    \"Score final = peso_pageindex * score_pageindex_normalizado + peso_vetorial * score_vetorial_normalizado\"\n",
        "                ),\n",
        "                \"pesos\": {\n",
        "                    \"peso_pageindex\": self.peso_pageindex,\n",
        "                    \"peso_vetorial\": self.peso_vetorial,\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "busca_hibrida = SistemaBuscaHibrida(peso_pageindex=0.7, peso_vetorial=0.3)\n",
        "rankeamento = busca_hibrida.rankear(\n",
        "    resultados_pageindex=[{\"id\": \"doc_1\", \"score\": 0.9}, {\"id\": \"doc_2\", \"score\": 0.7}],\n",
        "    resultados_vetoriais=[{\"id\": \"doc_2\", \"score\": 0.95}, {\"id\": \"doc_3\", \"score\": 0.6}],\n",
        ")\n",
        "rankeamento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat Jurídico Robusto\n",
        "\n",
        "Este fluxo simula um chat jurídico que:\n",
        "- Mantém histórico de contexto\n",
        "- Registra auditoria de cada interação\n",
        "- Retorna respostas com fontes rastreáveis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ChatJuridico:\n",
        "    def __init__(self, sistema: SistemaJuridicoUnificado):\n",
        "        self.sistema = sistema\n",
        "        self.historico: List[Dict[str, Any]] = []\n",
        "\n",
        "    def enviar(self, consulta: str, fontes: Optional[List[str]] = None) -> Dict[str, Any]:\n",
        "        contexto = {\"fontes\": fontes or []}\n",
        "        resposta = self.sistema.responder_consulta(consulta, contexto)\n",
        "        self.historico.append({\"consulta\": consulta, \"resposta\": resposta})\n",
        "        self.sistema.chatindex.registrar({\"consulta\": consulta, \"resposta\": resposta})\n",
        "        return resposta\n",
        "\n",
        "\n",
        "sistema = SistemaJuridicoUnificado(config)\n",
        "chat = ChatJuridico(sistema)\n",
        "\n",
        "chat.enviar(\"Quais são os requisitos para tutela de urgência?\", [\"STJ\", \"Planalto\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avaliação e testes de regressão\n",
        "\n",
        "Exemplos simples de métricas para validar a qualidade do ranking e detectar regressões.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def recall_at_k(resultados: List[str], relevantes: List[str], k: int = 5) -> float:\n",
        "    resultados_k = set(resultados[:k])\n",
        "    relevantes_set = set(relevantes)\n",
        "    if not relevantes_set:\n",
        "        return 0.0\n",
        "    return len(resultados_k & relevantes_set) / len(relevantes_set)\n",
        "\n",
        "\n",
        "def avaliacao_regressao(resultados: List[str], relevantes: List[str], limite: float = 0.6) -> Dict[str, Any]:\n",
        "    recall = recall_at_k(resultados, relevantes, k=5)\n",
        "    aprovado = recall >= limite\n",
        "    return {\"recall@5\": recall, \"limite\": limite, \"aprovado\": aprovado}\n",
        "\n",
        "\n",
        "avaliacao_regressao([\"doc_1\", \"doc_2\", \"doc_3\"], [\"doc_2\", \"doc_4\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Próximos passos\n",
        "\n",
        "1. Substituir os placeholders por integrações reais (Docling, PageIndex API).\n",
        "2. Implementar scraping com Playwright e rate limiting.\n",
        "3. Enriquecer o ChatIndex com histórico persistente.\n",
        "4. Adicionar testes de regressão e avaliação de qualidade.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}