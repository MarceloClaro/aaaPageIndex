{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sistema Jur√≠dico RAG Avan√ßado (Notebook Demo)\n",
        "\n",
        "Este notebook apresenta uma implementa√ß√£o de refer√™ncia baseada na arquitetura de 4 camadas descrita na documenta√ß√£o t√©cnica. Ele foca em:\n",
        "- Extra√ß√£o estruturada (Docling)\n",
        "- Indexa√ß√£o baseada em racioc√≠nio (PageIndex)\n",
        "- Gest√£o de contexto conversacional (ChatIndex)\n",
        "- Persist√™ncia audit√°vel (Google Drive)\n",
        "\n",
        "Inclui tamb√©m um chat jur√≠dico robusto com rastreabilidade e logs de auditoria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Depend√™ncias e configura√ß√£o\n",
        "\n",
        "Este notebook foi estruturado para rodar localmente ou em ambientes como Colab. Ajuste os caminhos e chaves conforme necess√°rio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Instala√ß√£o de depend√™ncias avan√ßadas (execute apenas uma vez)\n",
        "%pip install -q langchain langchain-openai langchain-groq langgraph chromadb pypdf sentence-transformers faiss-cpu\n",
        "%pip install -q \"mcp[cli]\" mcp[openai] nest-asyncio python-magic pdfplumber\n",
        "%pip install -q beautifulsoup4 requests playwright lxml html2text\n",
        "%pip install -q pymupdf pytesseract pillow\n",
        "!apt-get update && apt-get install -y poppler-utils tesseract-ocr-por > /dev/null\n",
        "!playwright install --with-deps chromium\n",
        "print(\"‚úÖ Pacotes instalados com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional\n",
        "import asyncio\n",
        "import requests\n",
        "from docling.document_converter import DocumentConverter\n",
        "from playwright.async_api import async_playwright\n",
        "import hashlib\n",
        "import json\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Configura√ß√£o base (ajuste conforme seu ambiente)\n",
        "@dataclass(frozen=True)\n",
        "class ConfigSistema:\n",
        "    pageindex_api_url: str = \"https://api.pageindex.ai/v1/index\"\n",
        "    pageindex_api_key: str = \"SUA_CHAVE_AQUI\"\n",
        "    chatindex_dir: Path = Path(\"./chatindex\")\n",
        "    drive_root: Path = Path(\"./Juridico_Unificado\")\n",
        "    audit_dir: Path = Path(\"./Juridico_Unificado/05_Auditoria\")\n",
        "    cache_dir: Path = Path(\"./cache_juridico\")\n",
        "    mcp_servers_dir: Path = Path(\"./Juridico_Unificado/04_Integracoes/mcp_servers\")\n",
        "    versao_fonte: str = \"v1\"\n",
        "    groq_api_key: str = os.environ.get(\"GROQ_API_KEY\", \"\")\n",
        "    groq_model_analise: str = \"llama-3.1-8b-instant\"\n",
        "    groq_model_resposta: str = \"llama-3.1-70b-versatile\"\n",
        "\n",
        "config = ConfigSistema()\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Configura√ß√£o de API Keys (preencha antes de usar LLMs)\n",
        "os.environ.setdefault(\"GROQ_API_KEY\", \"SUA_CHAVE_AQUI\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Como usar o sistema\n",
        "\n",
        "1. **Processar documentos:**\n",
        "   ```python\n",
        "   resultado = sistema.processar_documento(Path(\"seu_documento.pdf\"))\n",
        "   ```\n",
        "2. **Consultar o sistema:**\n",
        "   ```python\n",
        "   resposta = sistema.responder_consulta(\"sua pergunta jur√≠dica\", {\"fontes\": []})\n",
        "   ```\n",
        "3. **Gerar relat√≥rios:**\n",
        "   ```python\n",
        "   relatorio = sistema.auditoria.registrar_evento(\"relatorios\", {\"tipo\": \"snapshot\"})\n",
        "   ```\n",
        "4. **Acessar auditoria:**\n",
        "   ```python\n",
        "   sistema.auditoria.registrar_evento(\"consultas\", {\"tipo\": \"consulta\", \"consulta\": \"exemplo\"})\n",
        "   ```\n",
        "\n",
        "### üîß Configura√ß√µes importantes\n",
        "- **API Keys:** configure `GROQ_API_KEY` para respostas com LLM.\n",
        "- **Fontes:** scraping inclui STF, Planalto e STJ.\n",
        "- **Armazenamento:** tudo √© salvo no Google Drive de forma estruturada.\n",
        "- **Chunking:** preserva estrutura jur√≠dica e evita quebras inadequadas.\n",
        "\n",
        "### üöÄ Pr√≥ximos passos\n",
        "1. Configurar API do PageIndex para indexa√ß√£o em √°rvore real.\n",
        "2. Implementar servidores MCP locais para melhor integra√ß√£o.\n",
        "3. Adicionar mais fontes oficiais de scraping.\n",
        "4. Implementar cache distribu√≠do para melhor performance.\n",
        "5. Adicionar dashboard de monitoramento.\n",
        "\n",
        "### üìÅ Estrutura no Google Drive\n",
        "```text\n",
        "Juridico_Unificado/\n",
        "‚îú‚îÄ‚îÄ 01_PageIndex/          # √Årvores de documentos\n",
        "‚îú‚îÄ‚îÄ 02_ChatIndex/         # Hist√≥rico de conversas\n",
        "‚îú‚îÄ‚îÄ 03_Docling_Output/    # Extra√ß√µes estruturadas\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ raw_extractions/  # Extra√ß√µes brutas\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ structured_outputs/ # Extra√ß√µes processadas\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ chunks_semanticos/ # Chunks preservados\n",
        "‚îú‚îÄ‚îÄ 04_Integracoes/       # Dados cruzados\n",
        "‚îú‚îÄ‚îÄ 05_Auditoria/         # Logs e rastreabilidade\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ log_central.jsonl # Log principal\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ relatorios/       # Relat√≥rios gerados\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ consultas/        # Hist√≥rico de consultas\n",
        "‚îî‚îÄ‚îÄ indice_documentos.json # √çndice global\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Notas importantes\n",
        "- O scraping de fontes reais requer ajustes para sites espec√≠ficos.\n",
        "- A integra√ß√£o completa com PageIndex MCP requer servidor em execu√ß√£o.\n",
        "- Configure `GROQ_API_KEY` para respostas mais inteligentes.\n",
        "- Documentos muito grandes podem exigir ajustes no chunking.\n",
        "\n",
        "‚úÖ O sistema est√° pronto para processar documentos jur√≠dicos com:\n",
        "- Extra√ß√£o estrutural com Docling\n",
        "- Chunking sem√¢ntico que preserva contexto\n",
        "- Scraping de fontes oficiais\n",
        "- Armazenamento rastre√°vel no Google Drive\n",
        "- Auditoria completa de todas as opera√ß√µes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Exemplo: baixar PDF para processamento\n",
        "def baixar_pdf(url: str, destino: Path) -> Path:\n",
        "    destino.parent.mkdir(parents=True, exist_ok=True)\n",
        "    resposta = requests.get(url, timeout=30)\n",
        "    resposta.raise_for_status()\n",
        "    destino.write_bytes(resposta.content)\n",
        "    return destino\n",
        "\n",
        "pdf_url = \"https://arxiv.org/pdf/2103.15348.pdf\"\n",
        "pdf_path = baixar_pdf(pdf_url, Path(\"./cache_juridico/exemplo.pdf\"))\n",
        "print(f\"PDF salvo em: {pdf_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Camada 1: Orquestra√ß√£o\n",
        "\n",
        "A classe `SistemaJuridicoUnificado` atua como facade, coordenando extra√ß√£o, chunking, indexa√ß√£o, scraping e auditoria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Integra√ß√µes reais (Docling, PageIndex, scraping)\n",
        "\n",
        "Os componentes abaixo conectam o fluxo a extra√ß√£o real via Docling, indexa√ß√£o na API do PageIndex e scraping com Playwright, incluindo rate limiting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SistemaExtracaoDocling:\n",
        "    def __init__(self) -> None:\n",
        "        self.converter = DocumentConverter()\n",
        "\n",
        "    def extrair(self, documento_path: Path) -> Dict[str, Any]:\n",
        "        resultado = self.converter.convert(str(documento_path))\n",
        "        documento = resultado.document\n",
        "        texto = documento.export_to_text()\n",
        "        return {\n",
        "            \"documento_id\": documento_path.stem,\n",
        "            \"texto_completo\": texto,\n",
        "            \"metadata\": getattr(resultado, \"metadata\", {}),\n",
        "        }\n",
        "\n",
        "\n",
        "class PageIndexClient:\n",
        "    def __init__(self, api_url: str, api_key: str) -> None:\n",
        "        self.api_url = api_url\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def indexar(self, documento_id: str, chunks: List[str]) -> Dict[str, Any]:\n",
        "        resposta = requests.post(\n",
        "            self.api_url,\n",
        "            json={\"documento_id\": documento_id, \"chunks\": chunks},\n",
        "            headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n",
        "            timeout=30,\n",
        "        )\n",
        "        resposta.raise_for_status()\n",
        "        return resposta.json()\n",
        "\n",
        "\n",
        "class SistemaDownload:\n",
        "    def __init__(self, versao_fonte: str) -> None:\n",
        "        self.versao_fonte = versao_fonte\n",
        "\n",
        "    def registrar_fonte(self, origem: str) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"origem\": origem,\n",
        "            \"versao_fonte\": self.versao_fonte,\n",
        "            \"data_captura\": datetime.now().isoformat(),\n",
        "        }\n",
        "\n",
        "\n",
        "class SistemaScrapingJuridico:\n",
        "    def __init__(self, downloader: SistemaDownload, rate_limit_seconds: float = 1.0) -> None:\n",
        "        self.rate_limit_seconds = rate_limit_seconds\n",
        "        self.downloader = downloader\n",
        "\n",
        "    async def coletar_texto(self, url: str) -> Dict[str, Any]:\n",
        "        async with async_playwright() as playwright:\n",
        "            browser = await playwright.chromium.launch()\n",
        "            page = await browser.new_page()\n",
        "            await page.goto(url, wait_until=\"networkidle\")\n",
        "            await asyncio.sleep(self.rate_limit_seconds)\n",
        "            texto = await page.inner_text(\"body\")\n",
        "            await browser.close()\n",
        "            return {\"texto\": texto, \"metadata\": self.downloader.registrar_fonte(url)}\n",
        "\n",
        "\n",
        "class ChatIndexPersistente:\n",
        "    def __init__(self, storage_path: Path) -> None:\n",
        "        self.storage_path = storage_path\n",
        "        self.storage_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def registrar(self, conversa: Dict[str, Any]) -> None:\n",
        "        with self.storage_path.open(\"a\", encoding=\"utf-8\") as handle:\n",
        "            handle.write(json.dumps(conversa, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    def carregar(self) -> List[Dict[str, Any]]:\n",
        "        if not self.storage_path.exists():\n",
        "            return []\n",
        "        return [\n",
        "            json.loads(linha)\n",
        "            for linha in self.storage_path.read_text(encoding=\"utf-8\").splitlines()\n",
        "            if linha.strip()\n",
        "        ]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Exemplo: scraping simples de uma p√°gina\n",
        "sistema = SistemaJuridicoUnificado(config)\n",
        "texto_coletado = asyncio.run(sistema.scraper.coletar_texto(\"https://www.gov.br/\"))\n",
        "print(texto_coletado[\"metadata\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SistemaAuditoriaUnificado:\n",
        "    def __init__(self, audit_dir: Path):\n",
        "        self.audit_dir = audit_dir\n",
        "        self.audit_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.log_central: List[Dict[str, Any]] = []\n",
        "        self.hash_registry: Dict[str, Dict[str, str]] = {}\n",
        "\n",
        "    def registrar_evento(self, categoria: str, evento: Dict[str, Any]) -> str:\n",
        "        evento_id = f\"evt_{hashlib.md5(str(evento).encode()).hexdigest()[:10]}\"\n",
        "        evento[\"timestamp\"] = evento.get(\"timestamp\", datetime.now().isoformat())\n",
        "\n",
        "        if self.log_central:\n",
        "            evento[\"hash_anterior\"] = self.hash_registry[self.log_central[-1][\"evento_id\"]][\"hash\"]\n",
        "\n",
        "        hash_atual = hashlib.md5(json.dumps(evento, sort_keys=True).encode()).hexdigest()\n",
        "        self.hash_registry[evento_id] = {\"hash\": hash_atual, \"timestamp\": evento[\"timestamp\"]}\n",
        "\n",
        "        registro = {**evento, \"evento_id\": evento_id, \"hash_atual\": hash_atual}\n",
        "        self.log_central.append(registro)\n",
        "        self._persistir_log(categoria, registro)\n",
        "        return evento_id\n",
        "\n",
        "    def _persistir_log(self, categoria: str, registro: Dict[str, Any]) -> None:\n",
        "        destino = self.audit_dir / f\"{categoria}.jsonl\"\n",
        "        with destino.open(\"a\", encoding=\"utf-8\") as handle:\n",
        "            handle.write(json.dumps(registro, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "class SistemaJuridicoUnificado:\n",
        "    def __init__(self, config: ConfigSistema):\n",
        "        self.config = config\n",
        "        self.auditoria = SistemaAuditoriaUnificado(config.audit_dir)\n",
        "        self.extrator = SistemaExtracaoDocling()\n",
        "        self.pageindex = PageIndexClient(config.pageindex_api_url, config.pageindex_api_key)\n",
        "        self.downloader = SistemaDownload(config.versao_fonte)\n",
        "        self.scraper = SistemaScrapingJuridico(self.downloader)\n",
        "        self.chatindex = ChatIndexPersistente(config.chatindex_dir / \"historico.jsonl\")\n",
        "\n",
        "    def _chunking_semantico(self, texto: str, tamanho: int = 800) -> List[str]:\n",
        "        return [texto[i:i + tamanho] for i in range(0, len(texto), tamanho)]\n",
        "\n",
        "    def _extrair_versoes_fontes(self, metadados_fontes: List[Dict[str, Any]]) -> List[str]:\n",
        "    def _gerar_resposta_llm(self, consulta: str, fontes: List[str], metadados_fontes: List[Dict[str, Any]]) -> str:\n",
        "        if not self.config.groq_api_key or self.config.groq_api_key == \"SUA_CHAVE_AQUI\":\n",
        "            return \"\"\n",
        "        analise_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"Voc√™ √© um analista jur√≠dico. Extraia pontos-chave e contexto necess√°rio.\"),\n",
        "            (\"human\", \"Consulta: {consulta}\\nFontes: {fontes}\")\n",
        "        ])\n",
        "        resposta_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"Voc√™ √© um assistente jur√≠dico. Responda com base nos pontos-chave e fontes.\"),\n",
        "            (\"human\", \"Consulta: {consulta}\\nPontos-chave: {pontos}\\nMetadados: {metadados}\")\n",
        "        ])\n",
        "        llm_analise = ChatGroq(\n",
        "            groq_api_key=self.config.groq_api_key,\n",
        "            model=self.config.groq_model_analise,\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        llm_resposta = ChatGroq(\n",
        "            groq_api_key=self.config.groq_api_key,\n",
        "            model=self.config.groq_model_resposta,\n",
        "            temperature=0.3,\n",
        "        )\n",
        "        pontos = (analise_prompt | llm_analise | StrOutputParser()).invoke({\n",
        "            \"consulta\": consulta,\n",
        "            \"fontes\": \", \".join(fontes),\n",
        "        })\n",
        "        return (resposta_prompt | llm_resposta | StrOutputParser()).invoke({\n",
        "            \"consulta\": consulta,\n",
        "            \"pontos\": pontos,\n",
        "            \"metadados\": metadados_fontes,\n",
        "        })\n",
        "\n",
        "        return sorted({\n",
        "            item.get(\"versao_fonte\")\n",
        "            for item in metadados_fontes\n",
        "            if item.get(\"versao_fonte\")\n",
        "        })\n",
        "\n",
        "    def processar_documento(self, documento_path: Path) -> Dict[str, Any]:\n",
        "        self.auditoria.registrar_evento(\"processamento\", {\n",
        "            \"tipo\": \"inicio_processamento\",\n",
        "            \"documento\": documento_path.name,\n",
        "        })\n",
        "        extracao = self.extrator.extrair(documento_path)\n",
        "        chunks = self._chunking_semantico(extracao[\"texto_completo\"])\n",
        "        indexacao = self.pageindex.indexar(extracao[\"documento_id\"], chunks)\n",
        "        arvore = {\"documento_id\": documento_path.stem, \"estrutura_arvore\": {\"raiz\": {\"titulo\": documento_path.stem}}}\n",
        "\n",
        "        self.auditoria.registrar_evento(\"processamento\", {\n",
        "            \"tipo\": \"fim_processamento\",\n",
        "            \"documento\": documento_path.name,\n",
        "            \"chunks_gerados\": len(chunks),\n",
        "        })\n",
        "        return {\"extracao\": extracao, \"chunks\": chunks, \"arvore\": arvore, \"indexacao\": indexacao}\n",
        "\n",
        "    def responder_consulta(self, consulta: str, contexto: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        metadados_fontes = contexto.get(\"metadados_fontes\", [])\n",
        "        versoes_fontes = self._extrair_versoes_fontes(metadados_fontes)\n",
        "        self.auditoria.registrar_evento(\"consultas\", {\n",
        "            \"tipo\": \"consulta\",\n",
        "            \"consulta\": consulta,\n",
        "            \"versao_fonte\": versoes_fontes,\n",
        "        })\n",
        "        resposta_texto = self._gerar_resposta_llm(consulta, contexto.get(\"fontes\", []), metadados_fontes)\n",
        "        resposta = {\n",
        "            \"consulta\": consulta,\n",
        "            \"resposta\": resposta_texto or f\"Resumo jur√≠dico para: {consulta}\",\n",
        "            \"fontes\": contexto.get(\"fontes\", []),\n",
        "            \"metadados_fontes\": metadados_fontes,\n",
        "            \"versao_fonte\": versoes_fontes,\n",
        "        }\n",
        "        self.auditoria.registrar_evento(\"consultas\", {\n",
        "            \"tipo\": \"resposta\",\n",
        "            \"consulta\": consulta,\n",
        "            \"hash_resposta\": hashlib.md5(json.dumps(resposta, ensure_ascii=False).encode()).hexdigest(),\n",
        "            \"versao_fonte\": versoes_fontes,\n",
        "        })\n",
        "        return resposta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Camada 2: Busca h√≠brida e ranking\n",
        "\n",
        "A classe `SistemaBuscaHibrida` combina sinais do PageIndex e de embeddings vetoriais, com pesos configur√°veis e explicabilidade do ranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SistemaBuscaHibrida:\n",
        "    def __init__(self, peso_pageindex: float = 0.6, peso_vetorial: float = 0.4):\n",
        "        self.peso_pageindex = peso_pageindex\n",
        "        self.peso_vetorial = peso_vetorial\n",
        "\n",
        "    def _normalizar(self, resultados: list[dict[str, float]]) -> dict[str, float]:\n",
        "        if not resultados:\n",
        "            return {}\n",
        "        max_score = max(item.get(\"score\", 0.0) for item in resultados) or 1.0\n",
        "        return {item[\"id\"]: item.get(\"score\", 0.0) / max_score for item in resultados}\n",
        "\n",
        "    def rankear(self, resultados_pageindex: list[dict[str, float]], resultados_vetoriais: list[dict[str, float]]):\n",
        "        pageindex_norm = self._normalizar(resultados_pageindex)\n",
        "        vetorial_norm = self._normalizar(resultados_vetoriais)\n",
        "        ids = set(pageindex_norm) | set(vetorial_norm)\n",
        "        combinados = []\n",
        "\n",
        "        for doc_id in ids:\n",
        "            score_pageindex = pageindex_norm.get(doc_id, 0.0)\n",
        "            score_vetorial = vetorial_norm.get(doc_id, 0.0)\n",
        "            score_final = (\n",
        "                self.peso_pageindex * score_pageindex +\n",
        "                self.peso_vetorial * score_vetorial\n",
        "            )\n",
        "            combinados.append({\n",
        "                \"id\": doc_id,\n",
        "                \"score_final\": score_final,\n",
        "                \"score_pageindex\": score_pageindex,\n",
        "                \"score_vetorial\": score_vetorial,\n",
        "            })\n",
        "\n",
        "        combinados.sort(key=lambda item: item[\"score_final\"], reverse=True)\n",
        "        return {\n",
        "            \"resultados\": combinados,\n",
        "            \"explicabilidade\": {\n",
        "                \"criterios\": (\n",
        "                    \"Score final = peso_pageindex * score_pageindex_normalizado + peso_vetorial * score_vetorial_normalizado\"\n",
        "                ),\n",
        "                \"pesos\": {\n",
        "                    \"peso_pageindex\": self.peso_pageindex,\n",
        "                    \"peso_vetorial\": self.peso_vetorial,\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "busca_hibrida = SistemaBuscaHibrida(peso_pageindex=0.7, peso_vetorial=0.3)\n",
        "rankeamento = busca_hibrida.rankear(\n",
        "    resultados_pageindex=[{\"id\": \"doc_1\", \"score\": 0.9}, {\"id\": \"doc_2\", \"score\": 0.7}],\n",
        "    resultados_vetoriais=[{\"id\": \"doc_2\", \"score\": 0.95}, {\"id\": \"doc_3\", \"score\": 0.6}],\n",
        ")\n",
        "rankeamento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat Jur√≠dico Robusto\n",
        "\n",
        "Este fluxo simula um chat jur√≠dico que:\n",
        "- Mant√©m hist√≥rico de contexto\n",
        "- Registra auditoria de cada intera√ß√£o\n",
        "- Retorna respostas com fontes rastre√°veis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ChatJuridico:\n",
        "    def __init__(self, sistema: SistemaJuridicoUnificado):\n",
        "        self.sistema = sistema\n",
        "        self.historico: List[Dict[str, Any]] = []\n",
        "\n",
        "    def enviar(self, consulta: str, fontes: Optional[List[str]] = None) -> Dict[str, Any]:\n",
        "        fontes_list = fontes or []\n",
        "        metadados_fontes = [\n",
        "            self.sistema.downloader.registrar_fonte(fonte)\n",
        "            for fonte in fontes_list\n",
        "        ]\n",
        "        contexto = {\"fontes\": fontes_list, \"metadados_fontes\": metadados_fontes}\n",
        "        resposta = self.sistema.responder_consulta(consulta, contexto)\n",
        "        self.historico.append({\"consulta\": consulta, \"resposta\": resposta})\n",
        "        self.sistema.chatindex.registrar({\"consulta\": consulta, \"resposta\": resposta})\n",
        "        return resposta\n",
        "\n",
        "\n",
        "sistema = SistemaJuridicoUnificado(config)\n",
        "chat = ChatJuridico(sistema)\n",
        "\n",
        "chat.enviar(\"Quais s√£o os requisitos para tutela de urg√™ncia?\", [\"STJ\", \"Planalto\"])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Conversa√ß√£o de exemplo com o chat jur√≠dico\n",
        "sistema = SistemaJuridicoUnificado(config)\n",
        "chat = ChatJuridico(sistema)\n",
        "\n",
        "chat.enviar(\"Quais s√£o os requisitos para tutela de urg√™ncia?\", [\"STJ\", \"Planalto\"])\n",
        "chat.enviar(\"Qual a base legal da responsabilidade civil objetiva?\", [\"STF\", \"Planalto\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avalia√ß√£o e testes de regress√£o\n",
        "\n",
        "Exemplos simples de m√©tricas para validar a qualidade do ranking e detectar regress√µes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def recall_at_k(resultados: List[str], relevantes: List[str], k: int = 5) -> float:\n",
        "    resultados_k = set(resultados[:k])\n",
        "    relevantes_set = set(relevantes)\n",
        "    if not relevantes_set:\n",
        "        return 0.0\n",
        "    return len(resultados_k & relevantes_set) / len(relevantes_set)\n",
        "\n",
        "\n",
        "def avaliacao_regressao(resultados: List[str], relevantes: List[str], limite: float = 0.6) -> Dict[str, Any]:\n",
        "    recall = recall_at_k(resultados, relevantes, k=5)\n",
        "    aprovado = recall >= limite\n",
        "    return {\"recall@5\": recall, \"limite\": limite, \"aprovado\": aprovado}\n",
        "\n",
        "\n",
        "avaliacao_regressao([\"doc_1\", \"doc_2\", \"doc_3\"], [\"doc_2\", \"doc_4\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pr√≥ximos passos\n",
        "\n",
        "1. Substituir os placeholders por integra√ß√µes reais (Docling, PageIndex API).\n",
        "2. Implementar scraping com Playwright e rate limiting.\n",
        "3. Enriquecer o ChatIndex com hist√≥rico persistente.\n",
        "4. Adicionar testes de regress√£o e avalia√ß√£o de qualidade.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}