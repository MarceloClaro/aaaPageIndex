{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================\n# CÃ‰LULA 1: ARQUITETURA PRINCIPAL UNIFICADA\n# ============================================\n\"\"\"\nARQUITETURA UNIFICADA - 4 CAMADAS INTEGRADAS\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CAMADA DE ORQUESTRAÃ‡ÃƒO                    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚         Sistema JurÃ­dico Unificado (Orchestrator)    â”‚   â”‚\nâ”‚  â”‚  â€¢ Coordena todos os componentes                     â”‚   â”‚\nâ”‚  â”‚  â€¢ Gerenciamento de fluxos de trabalho               â”‚   â”‚\nâ”‚  â”‚  â€¢ Balanceamento de carga entre MCPs                 â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CAMADA DE SERVIÃ‡OS MCP                   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚ PageIndex   â”‚  â”‚   ChatIndex  â”‚  â”‚    Docling       â”‚   â”‚\nâ”‚  â”‚    MCP      â”‚  â”‚      MCP     â”‚  â”‚      MCP         â”‚   â”‚\nâ”‚  â”‚ (Ãrvores)   â”‚  â”‚ (Conversas)  â”‚  â”‚ (ExtraÃ§Ã£o)       â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚  â€¢ RaciocÃ­nio     â€¢ MemÃ³ria de      â€¢ OCR AvanÃ§ado         â”‚\nâ”‚    em Ã¡rvore        longa conversa  â€¢ Estrutura preservada â”‚\nâ”‚  â€¢ Busca sem        â€¢ B+ Trees      â€¢ Multi-modal          â”‚\nâ”‚    vetores        â€¢ TÃ³picos         â€¢ Tabelas/Imagens      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CAMADA DE PROCESSAMENTO                  â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚          Pipeline Inteligente de Documentos          â”‚   â”‚\nâ”‚  â”‚  1. ExtraÃ§Ã£o Docling â†’ 2. IndexaÃ§Ã£o PageIndex â†’      â”‚   â”‚\nâ”‚  â”‚  3. Chunking SemÃ¢ntico â†’ 4. ChatIndex Storage â†’      â”‚   â”‚\nâ”‚  â”‚  5. MCP Integration â†’ 6. Auditoria                   â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                 CAMADA DE PERSISTÃŠNCIA UNIFICADA            â”‚\nâ”‚                    GOOGLE DRIVE ESTRUTURADO                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚  Juridico_Unificado/                                 â”‚\nâ”‚  â”‚  â”œâ”€â”€ 00_MCP_Servers/     # ConfiguraÃ§Ãµes MCP         â”‚\nâ”‚  â”‚  â”œâ”€â”€ 01_PageIndex/       # Ãrvores de documentos     â”‚\nâ”‚  â”‚  â”œâ”€â”€ 02_ChatIndex/       # Ãrvores de conversas      â”‚\nâ”‚  â”‚  â”œâ”€â”€ 03_Docling_Output/  # ExtraÃ§Ãµes estruturadas    â”‚\nâ”‚  â”‚  â”œâ”€â”€ 04_Integracoes/     # Dados cruzados            â”‚\nâ”‚  â”‚  â””â”€â”€ 05_Auditoria/       # Logs unificados           â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sistema JurÃ­dico RAG AvanÃ§ado (Notebook Demo)\n",
        "\n",
        "Este notebook apresenta uma implementaÃ§Ã£o de referÃªncia baseada na arquitetura de 4 camadas descrita na documentaÃ§Ã£o tÃ©cnica. Ele foca em:\n",
        "- ExtraÃ§Ã£o estruturada (Docling)\n",
        "- IndexaÃ§Ã£o baseada em raciocÃ­nio (PageIndex)\n",
        "- GestÃ£o de contexto conversacional (ChatIndex)\n",
        "- PersistÃªncia auditÃ¡vel (Google Drive)\n",
        "\n",
        "Inclui tambÃ©m um chat jurÃ­dico robusto com rastreabilidade e logs de auditoria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DependÃªncias e configuraÃ§Ã£o\n",
        "\n",
        "Este notebook foi estruturado para rodar localmente ou em ambientes como Colab. Ajuste os caminhos e chaves conforme necessÃ¡rio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# CÃ‰LULA 1: INSTALAÃ‡ÃƒO E CONFIGURAÃ‡ÃƒO COMPLETA (execute apenas uma vez)\n",
        "print(\"ðŸš€ INICIALIZANDO SISTEMA JURÃDICO UNIFICADO AVANÃ‡ADO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nðŸ“¦ INSTALANDO DEPENDÃŠNCIAS...\")\n",
        "%pip install -qU langchain langchain-groq\n",
        "%pip install -qU \"docling[all]\"\n",
        "%pip install -qU \"mcp[cli]\" nest-asyncio\n",
        "%pip install -qU beautifulsoup4 requests lxml html2text\n",
        "%pip install -qU pymupdf pytesseract pillow\n",
        "%pip install -qU aiohttp google-api-python-client google-auth\n",
        "%pip install -qU scikit-learn networkx pandas numpy\n",
        "%pip install -qU sentence-transformers\n",
        "!apt-get update -q\n",
        "!apt-get install -qy tesseract-ocr tesseract-ocr-por poppler-utils > /dev/null\n",
        "!playwright install --with-deps chromium\n",
        "print(\"âœ… Pacotes instalados com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional\n",
        "import asyncio\n",
        "import requests\n",
        "from docling.document_converter import DocumentConverter\n",
        "from playwright.async_api import async_playwright\n",
        "import hashlib\n",
        "import json\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# ConfiguraÃ§Ã£o base (ajuste conforme seu ambiente)\n",
        "@dataclass(frozen=True)\n",
        "class ConfigSistema:\n",
        "    pageindex_api_url: str = \"https://api.pageindex.ai/v1/index\"\n",
        "    pageindex_api_key: str = \"SUA_CHAVE_AQUI\"\n",
        "    chatindex_dir: Path = Path(\"./chatindex\")\n",
        "    drive_root: Path = Path(\"./Juridico_Unificado\")\n",
        "    audit_dir: Path = Path(\"./Juridico_Unificado/05_Auditoria\")\n",
        "    cache_dir: Path = Path(\"./cache_juridico\")\n",
        "    mcp_servers_dir: Path = Path(\"./Juridico_Unificado/04_Integracoes/mcp_servers\")\n",
        "    versao_fonte: str = \"v1\"\n",
        "    groq_api_key: str = os.environ.get(\"GROQ_API_KEY\", \"\")\n",
        "    groq_model_analise: str = \"llama-3.1-8b-instant\"\n",
        "    groq_model_resposta: str = \"llama-3.1-70b-versatile\"\n",
        "\n",
        "config = ConfigSistema()\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ConfiguraÃ§Ã£o do ambiente e chaves (preencha antes de usar LLMs)\n",
        "os.environ.setdefault(\"PAGEINDEX_API_KEY\", \"SUA_CHAVE_AQUI\")\n",
        "os.environ.setdefault(\"GROQ_API_KEY\", \"SUA_CHAVE_AQUI\")\n",
        "\n",
        "# DiretÃ³rios padrÃ£o (ajuste para /content/drive/... se estiver no Colab)\n",
        "drive_root = Path(\"/content/drive/MyDrive/Juridico_Unificado\")\n",
        "drive_root.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CÃ‰LULA 2: SISTEMA DE AUDITORIA E RASTREABILIDADE\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸ“Š SISTEMA DE AUDITORIA E RASTREABILIDADE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from typing import Dict, List, Any, Optional, Tuple, Set\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from enum import Enum\n",
        "import hashlib\n",
        "import uuid\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "CONFIG = {\n",
        "    \"audit_dir\": str(config.audit_dir),\n",
        "    \"base_path\": str(config.drive_root),\n",
        "}\n",
        "\n",
        "class EventoTipo(Enum):\n",
        "    \"\"\"Tipos de eventos auditÃ¡veis\"\"\"\n",
        "    INICIO_PROCESSAMENTO = \"inicio_processamento\"\n",
        "    DOWNLOAD_DOCUMENTO = \"download_documento\"\n",
        "    EXTRACAO_DOCLING = \"extracao_docling\"\n",
        "    INDEXACAO_PAGEINDEX = \"indexacao_pageindex\"\n",
        "    CHUNKING_SEMANTICO = \"chunking_semantico\"\n",
        "    ARMAZENAMENTO_CHATINDEX = \"armazenamento_chatindex\"\n",
        "    INTEGRACAO_MCP = \"integracao_mcp\"\n",
        "    CONSULTA_USUARIO = \"consulta_usuario\"\n",
        "    RESPOSTA_GERADA = \"resposta_gerada\"\n",
        "    ERRO_PROCESSAMENTO = \"erro_processamento\"\n",
        "    BACKUP_DRIVE = \"backup_drive\"\n",
        "    VERIFICACAO_INTEGRIDADE = \"verificacao_integridade\"\n",
        "\n",
        "@dataclass\n",
        "class EventoAuditoria:\n",
        "    \"\"\"Estrutura de evento auditÃ¡vel imutÃ¡vel\"\"\"\n",
        "    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n",
        "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "    tipo: EventoTipo = EventoTipo.INICIO_PROCESSAMENTO\n",
        "    componente: str = \"\"\n",
        "    acao: str = \"\"\n",
        "    detalhes: Dict[str, Any] = field(default_factory=dict)\n",
        "    hash_anterior: str = \"\"\n",
        "    hash_atual: str = \"\"\n",
        "    arquivos_relacionados: List[str] = field(default_factory=list)\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    def calcular_hash(self) -> str:\n",
        "        \"\"\"Calcula hash SHA-256 do evento\"\"\"\n",
        "        dados = f\"{self.timestamp}{self.tipo.value}{self.componente}{self.acao}{json.dumps(self.detalhes, sort_keys=True)}\"\n",
        "        return hashlib.sha256(dados.encode()).hexdigest()\n",
        "\n",
        "class SistemaAuditoriaUnificado:\n",
        "    \"\"\"\n",
        "    Sistema centralizado de auditoria com rastreabilidade completa\n",
        "    Implementa cadeia de hashes imutÃ¡vel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.audit_dir = Path(config[\"audit_dir\"])\n",
        "        self.log_central = []\n",
        "        self.hash_cadeia = \"\"\n",
        "\n",
        "        # Inicializar estrutura de auditoria\n",
        "        self._inicializar_auditoria()\n",
        "\n",
        "    def _inicializar_auditoria(self):\n",
        "        \"\"\"Inicializa sistema de auditoria\"\"\"\n",
        "        # Criar estrutura de logs\n",
        "        (self.audit_dir / \"logs_diarios\").mkdir(exist_ok=True)\n",
        "        (self.audit_dir / \"backups_hash\").mkdir(exist_ok=True)\n",
        "        (self.audit_dir / \"relatorios\").mkdir(exist_ok=True)\n",
        "\n",
        "        # Arquivo de log principal (append-only)\n",
        "        self.log_principal = self.audit_dir / \"auditoria_principal.jsonl\"\n",
        "\n",
        "        # Arquivo de Ã­ndices para busca rÃ¡pida\n",
        "        self.indice_eventos = self.audit_dir / \"indice_eventos.json\"\n",
        "\n",
        "        # Inicializar cadeia de hashes\n",
        "        evento_genese = EventoAuditoria(\n",
        "            tipo=EventoTipo.INICIO_PROCESSAMENTO,\n",
        "            componente=\"SistemaAuditoria\",\n",
        "            acao=\"InicializaÃ§Ã£o do sistema\",\n",
        "            detalhes={\"versao\": \"1.0.0\", \"configuracao\": self.config},\n",
        "            hash_anterior=\"0\" * 64,\n",
        "        )\n",
        "        evento_genese.hash_atual = evento_genese.calcular_hash()\n",
        "        self.hash_cadeia = evento_genese.hash_atual\n",
        "\n",
        "        # Registrar evento de gÃªnese\n",
        "        self._registrar_evento_persistente(evento_genese)\n",
        "\n",
        "        print(f\"âœ… Auditoria inicializada | Hash inicial: {self.hash_cadeia[:16]}...\")\n",
        "\n",
        "    def registrar_evento(self, evento: EventoAuditoria) -> str:\n",
        "        \"\"\"\n",
        "        Registra evento com cadeia de hashes imutÃ¡vel\n",
        "        \"\"\"\n",
        "        # Vincular ao hash anterior\n",
        "        evento.hash_anterior = self.hash_cadeia\n",
        "\n",
        "        # Calcular hash atual\n",
        "        evento.hash_atual = evento.calcular_hash()\n",
        "\n",
        "        # Atualizar cadeia\n",
        "        self.hash_cadeia = evento.hash_atual\n",
        "\n",
        "        # Persistir evento\n",
        "        self._registrar_evento_persistente(evento)\n",
        "\n",
        "        # Atualizar Ã­ndice\n",
        "        self._atualizar_indice(evento)\n",
        "\n",
        "        # Backup periÃ³dico\n",
        "        if len(self.log_central) % 100 == 0:\n",
        "            self._criar_backup_hash()\n",
        "\n",
        "        return evento.id\n",
        "\n",
        "    def _registrar_evento_persistente(self, evento: EventoAuditoria):\n",
        "        \"\"\"Persiste evento em mÃºltiplos formatos\"\"\"\n",
        "\n",
        "        # 1. Log principal (JSONL)\n",
        "        with open(self.log_principal, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(asdict(evento), ensure_ascii=False, default=str) + \"\\n\")\n",
        "\n",
        "        # 2. Log diÃ¡rio separado por data\n",
        "        data_str = evento.timestamp[:10]  # YYYY-MM-DD\n",
        "        log_diario = self.audit_dir / \"logs_diarios\" / f\"{data_str}.jsonl\"\n",
        "        with open(log_diario, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(asdict(evento), ensure_ascii=False, default=str) + \"\\n\")\n",
        "\n",
        "        # 3. Armazenar em memÃ³ria (limitado)\n",
        "        self.log_central.append(evento)\n",
        "        if len(self.log_central) > 1000:\n",
        "            self.log_central = self.log_central[-1000:]\n",
        "\n",
        "    def _atualizar_indice(self, evento: EventoAuditoria):\n",
        "        \"\"\"Atualiza Ã­ndice de eventos para busca rÃ¡pida\"\"\"\n",
        "\n",
        "        indice_path = self.indice_eventos\n",
        "        indice = {}\n",
        "\n",
        "        if indice_path.exists():\n",
        "            try:\n",
        "                indice = json.loads(indice_path.read_text())\n",
        "            except:\n",
        "                indice = {}\n",
        "\n",
        "        # Adicionar ao Ã­ndice\n",
        "        if evento.tipo.value not in indice:\n",
        "            indice[evento.tipo.value] = []\n",
        "\n",
        "        entrada_indice = {\n",
        "            \"id\": evento.id,\n",
        "            \"timestamp\": evento.timestamp,\n",
        "            \"componente\": evento.componente,\n",
        "            \"hash\": evento.hash_atual[:16],\n",
        "        }\n",
        "\n",
        "        indice[evento.tipo.value].append(entrada_indice)\n",
        "\n",
        "        # Manter Ã­ndice limitado\n",
        "        for tipo in indice:\n",
        "            indice[tipo] = indice[tipo][-1000:]\n",
        "\n",
        "        # Salvar Ã­ndice\n",
        "        indice_path.write_text(json.dumps(indice, indent=2, ensure_ascii=False, default=str))\n",
        "\n",
        "    def _criar_backup_hash(self):\n",
        "        \"\"\"Cria backup da cadeia de hashes\"\"\"\n",
        "        backup_file = self.audit_dir / \"backups_hash\" / f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "        backup_data = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"hash_cadeia\": self.hash_cadeia,\n",
        "            \"total_eventos\": len(self.log_central),\n",
        "            \"ultimos_10_hashes\": [e.hash_atual[:16] for e in self.log_central[-10:]],\n",
        "        }\n",
        "\n",
        "        backup_file.write_text(json.dumps(backup_data, indent=2, ensure_ascii=False, default=str))\n",
        "\n",
        "    def gerar_relatorio_rastreabilidade(self, evento_id: str = None, consulta_id: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Gera relatÃ³rio completo de rastreabilidade\n",
        "        \"\"\"\n",
        "\n",
        "        relatorio = {\n",
        "            \"metadata\": {\n",
        "                \"gerado_em\": datetime.now().isoformat(),\n",
        "                \"sistema\": \"Juridico_Unificado\",\n",
        "                \"hash_cadeia_atual\": self.hash_cadeia,\n",
        "            },\n",
        "            \"cadeia_eventos\": [],\n",
        "            \"integridade_verificada\": True,\n",
        "            \"arquivos_relacionados\": [],\n",
        "            \"timeline_detalhada\": [],\n",
        "        }\n",
        "\n",
        "        # Buscar eventos relacionados\n",
        "        eventos_relacionados = []\n",
        "\n",
        "        if evento_id:\n",
        "            # Buscar por ID especÃ­fico\n",
        "            for evento in self.log_central:\n",
        "                if evento.id == evento_id:\n",
        "                    eventos_relacionados.append(evento)\n",
        "                    break\n",
        "\n",
        "        if consulta_id:\n",
        "            # Buscar todos eventos de uma consulta\n",
        "            for evento in self.log_central:\n",
        "                if \"consulta_id\" in evento.detalhes and evento.detalhes[\"consulta_id\"] == consulta_id:\n",
        "                    eventos_relacionados.append(evento)\n",
        "\n",
        "        # Reconstruir cadeia\n",
        "        if eventos_relacionados:\n",
        "            evento_inicial = eventos_relacionados[0]\n",
        "            hash_atual = evento_inicial.hash_anterior\n",
        "\n",
        "            # Buscar cadeia completa\n",
        "            while hash_atual != \"0\" * 64:\n",
        "                for evento in reversed(self.log_central):\n",
        "                    if evento.hash_atual == hash_atual:\n",
        "                        relatorio[\"cadeia_eventos\"].append(asdict(evento))\n",
        "                        hash_atual = evento.hash_anterior\n",
        "                        break\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # Adicionar eventos relacionados\n",
        "            relatorio[\"cadeia_eventos\"].extend([asdict(e) for e in eventos_relacionados])\n",
        "\n",
        "            # Coletar arquivos\n",
        "            for evento in eventos_relacionados:\n",
        "                relatorio[\"arquivos_relacionados\"].extend(evento.arquivos_relacionados)\n",
        "\n",
        "            # Timeline detalhada\n",
        "            eventos_ordenados = sorted(eventos_relacionados, key=lambda x: x.timestamp)\n",
        "            for evento in eventos_ordenados:\n",
        "                relatorio[\"timeline_detalhada\"].append({\n",
        "                    \"timestamp\": evento.timestamp,\n",
        "                    \"tipo\": evento.tipo.value,\n",
        "                    \"acao\": evento.acao,\n",
        "                    \"duracao\": evento.detalhes.get(\"duracao_ms\", 0),\n",
        "                })\n",
        "\n",
        "        # Verificar integridade da cadeia\n",
        "        relatorio[\"integridade_verificada\"] = self._verificar_integridade_cadeia()\n",
        "\n",
        "        # Salvar relatÃ³rio\n",
        "        relatorio_path = self.audit_dir / \"relatorios\" / f\"rastreabilidade_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        relatorio_path.write_text(json.dumps(relatorio, indent=2, ensure_ascii=False, default=str))\n",
        "\n",
        "        return relatorio\n",
        "\n",
        "    def _verificar_integridade_cadeia(self) -> bool:\n",
        "        \"\"\"Verifica integridade da cadeia de hashes\"\"\"\n",
        "\n",
        "        if len(self.log_central) < 2:\n",
        "            return True\n",
        "\n",
        "        for i in range(1, len(self.log_central)):\n",
        "            evento_atual = self.log_central[i]\n",
        "            evento_anterior = self.log_central[i - 1]\n",
        "\n",
        "            if evento_atual.hash_anterior != evento_anterior.hash_atual:\n",
        "                print(f\"âš ï¸  ViolaÃ§Ã£o de integridade na posiÃ§Ã£o {i}\")\n",
        "                print(f\"   Hash anterior esperado: {evento_anterior.hash_atual[:16]}\")\n",
        "                print(f\"   Hash anterior encontrado: {evento_atual.hash_anterior[:16]}\")\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def monitorar_performance(self, periodo_horas: int = 24) -> Dict[str, Any]:\n",
        "        \"\"\"Monitora performance do sistema\"\"\"\n",
        "\n",
        "        cutoff_time = datetime.now() - timedelta(hours=periodo_horas)\n",
        "\n",
        "        eventos_periodo = [\n",
        "            e for e in self.log_central\n",
        "            if datetime.fromisoformat(e.timestamp.replace(\"Z\", \"+00:00\")) > cutoff_time\n",
        "        ]\n",
        "\n",
        "        metricas = {\n",
        "            \"total_eventos\": len(eventos_periodo),\n",
        "            \"por_tipo\": {},\n",
        "            \"tempo_medio_resposta\": 0,\n",
        "            \"taxa_erros\": 0,\n",
        "            \"componentes_ativos\": set(),\n",
        "        }\n",
        "\n",
        "        tempos_resposta = []\n",
        "        erros = 0\n",
        "\n",
        "        for evento in eventos_periodo:\n",
        "            # Contagem por tipo\n",
        "            tipo = evento.tipo.value\n",
        "            metricas[\"por_tipo\"][tipo] = metricas[\"por_tipo\"].get(tipo, 0) + 1\n",
        "\n",
        "            # Coletar componentes\n",
        "            metricas[\"componentes_ativos\"].add(evento.componente)\n",
        "\n",
        "            # Tempos de resposta\n",
        "            if \"duracao_ms\" in evento.detalhes:\n",
        "                tempos_resposta.append(evento.detalhes[\"duracao_ms\"])\n",
        "\n",
        "            # Contar erros\n",
        "            if evento.tipo == EventoTipo.ERRO_PROCESSAMENTO:\n",
        "                erros += 1\n",
        "\n",
        "        # Calcular mÃ©tricas\n",
        "        if tempos_resposta:\n",
        "            metricas[\"tempo_medio_resposta\"] = sum(tempos_resposta) / len(tempos_resposta)\n",
        "\n",
        "        if eventos_periodo:\n",
        "            metricas[\"taxa_erros\"] = erros / len(eventos_periodo)\n",
        "\n",
        "        metricas[\"componentes_ativos\"] = list(metricas[\"componentes_ativos\"])\n",
        "\n",
        "        return metricas\n",
        "\n",
        "# Inicializar sistema de auditoria\n",
        "auditoria = SistemaAuditoriaUnificado(CONFIG)\n",
        "\n",
        "print(\"âœ… Sistema de auditoria inicializado com cadeia de hashes imutÃ¡vel\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Como usar o sistema\n",
        "\n",
        "1. **Processar documentos:**\n",
        "   ```python\n",
        "   resultado = sistema.processar_documento(Path(\"seu_documento.pdf\"))\n",
        "   ```\n",
        "2. **Consultar o sistema:**\n",
        "   ```python\n",
        "   resposta = sistema.responder_consulta(\"sua pergunta jurÃ­dica\", {\"fontes\": []})\n",
        "   ```\n",
        "3. **Gerar relatÃ³rios:**\n",
        "   ```python\n",
        "   relatorio = sistema.auditoria.registrar_evento(\"relatorios\", {\"tipo\": \"snapshot\"})\n",
        "   ```\n",
        "4. **Acessar auditoria:**\n",
        "   ```python\n",
        "   sistema.auditoria.registrar_evento(\"consultas\", {\"tipo\": \"consulta\", \"consulta\": \"exemplo\"})\n",
        "   ```\n",
        "\n",
        "### ðŸ”§ ConfiguraÃ§Ãµes importantes\n",
        "- **API Keys:** configure `GROQ_API_KEY` para respostas com LLM.\n",
        "- **Fontes:** scraping inclui STF, Planalto e STJ.\n",
        "- **Armazenamento:** tudo Ã© salvo no Google Drive de forma estruturada.\n",
        "- **Chunking:** preserva estrutura jurÃ­dica e evita quebras inadequadas.\n",
        "\n",
        "### ðŸš€ PrÃ³ximos passos\n",
        "1. Configurar API do PageIndex para indexaÃ§Ã£o em Ã¡rvore real.\n",
        "2. Implementar servidores MCP locais para melhor integraÃ§Ã£o.\n",
        "3. Adicionar mais fontes oficiais de scraping.\n",
        "4. Implementar cache distribuÃ­do para melhor performance.\n",
        "5. Adicionar dashboard de monitoramento.\n",
        "\n",
        "### ðŸ“ Estrutura no Google Drive\n",
        "```text\n",
        "Juridico_Unificado/\n",
        "â”œâ”€â”€ 01_PageIndex/          # Ãrvores de documentos\n",
        "â”œâ”€â”€ 02_ChatIndex/         # HistÃ³rico de conversas\n",
        "â”œâ”€â”€ 03_Docling_Output/    # ExtraÃ§Ãµes estruturadas\n",
        "â”‚   â”œâ”€â”€ raw_extractions/  # ExtraÃ§Ãµes brutas\n",
        "â”‚   â”œâ”€â”€ structured_outputs/ # ExtraÃ§Ãµes processadas\n",
        "â”‚   â””â”€â”€ chunks_semanticos/ # Chunks preservados\n",
        "â”œâ”€â”€ 04_Integracoes/       # Dados cruzados\n",
        "â”œâ”€â”€ 05_Auditoria/         # Logs e rastreabilidade\n",
        "â”‚   â”œâ”€â”€ log_central.jsonl # Log principal\n",
        "â”‚   â”œâ”€â”€ relatorios/       # RelatÃ³rios gerados\n",
        "â”‚   â””â”€â”€ consultas/        # HistÃ³rico de consultas\n",
        "â””â”€â”€ indice_documentos.json # Ãndice global\n",
        "```\n",
        "\n",
        "### âš ï¸ Notas importantes\n",
        "- O scraping de fontes reais requer ajustes para sites especÃ­ficos.\n",
        "- A integraÃ§Ã£o completa com PageIndex MCP requer servidor em execuÃ§Ã£o.\n",
        "- Configure `GROQ_API_KEY` para respostas mais inteligentes.\n",
        "- Documentos muito grandes podem exigir ajustes no chunking.\n",
        "\n",
        "âœ… O sistema estÃ¡ pronto para processar documentos jurÃ­dicos com:\n",
        "- ExtraÃ§Ã£o estrutural com Docling\n",
        "- Chunking semÃ¢ntico que preserva contexto\n",
        "- Scraping de fontes oficiais\n",
        "- Armazenamento rastreÃ¡vel no Google Drive\n",
        "- Auditoria completa de todas as operaÃ§Ãµes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Exemplo: baixar PDF para processamento\n",
        "def baixar_pdf(url: str, destino: Path) -> Path:\n",
        "    destino.parent.mkdir(parents=True, exist_ok=True)\n",
        "    resposta = requests.get(url, timeout=30)\n",
        "    resposta.raise_for_status()\n",
        "    destino.write_bytes(resposta.content)\n",
        "    return destino\n",
        "\n",
        "pdf_url = \"https://arxiv.org/pdf/2103.15348.pdf\"\n",
        "pdf_path = baixar_pdf(pdf_url, Path(\"./cache_juridico/exemplo.pdf\"))\n",
        "print(f\"PDF salvo em: {pdf_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Camada 1: OrquestraÃ§Ã£o\n",
        "\n",
        "A classe `SistemaJuridicoUnificado` atua como facade, coordenando extraÃ§Ã£o, chunking, indexaÃ§Ã£o, scraping e auditoria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IntegraÃ§Ãµes reais (Docling, PageIndex, scraping)\n",
        "\n",
        "Os componentes abaixo conectam o fluxo a extraÃ§Ã£o real via Docling, indexaÃ§Ã£o na API do PageIndex e scraping com Playwright, incluindo rate limiting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SistemaExtracaoDocling:\n",
        "    def __init__(self) -> None:\n",
        "        self.converter = DocumentConverter()\n",
        "\n",
        "    def extrair(self, documento_path: Path) -> Dict[str, Any]:\n",
        "        resultado = self.converter.convert(str(documento_path))\n",
        "        documento = resultado.document\n",
        "        texto = documento.export_to_text()\n",
        "        return {\n",
        "            \"documento_id\": documento_path.stem,\n",
        "            \"texto_completo\": texto,\n",
        "            \"metadata\": getattr(resultado, \"metadata\", {}),\n",
        "        }\n",
        "\n",
        "\n",
        "class PageIndexClient:\n",
        "    def __init__(self, api_url: str, api_key: str) -> None:\n",
        "        self.api_url = api_url\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def indexar(self, documento_id: str, chunks: List[str]) -> Dict[str, Any]:\n",
        "        resposta = requests.post(\n",
        "            self.api_url,\n",
        "            json={\"documento_id\": documento_id, \"chunks\": chunks},\n",
        "            headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n",
        "            timeout=30,\n",
        "        )\n",
        "        resposta.raise_for_status()\n",
        "        return resposta.json()\n",
        "\n",
        "\n",
        "class SistemaDownload:\n",
        "    def __init__(self, versao_fonte: str) -> None:\n",
        "        self.versao_fonte = versao_fonte\n",
        "\n",
        "    def registrar_fonte(self, origem: str) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"origem\": origem,\n",
        "            \"versao_fonte\": self.versao_fonte,\n",
        "            \"data_captura\": datetime.now().isoformat(),\n",
        "        }\n",
        "\n",
        "\n",
        "class SistemaScrapingJuridico:\n",
        "    def __init__(self, downloader: SistemaDownload, rate_limit_seconds: float = 1.0) -> None:\n",
        "        self.rate_limit_seconds = rate_limit_seconds\n",
        "        self.downloader = downloader\n",
        "\n",
        "    async def coletar_texto(self, url: str) -> Dict[str, Any]:\n",
        "        async with async_playwright() as playwright:\n",
        "            browser = await playwright.chromium.launch()\n",
        "            page = await browser.new_page()\n",
        "            await page.goto(url, wait_until=\"networkidle\")\n",
        "            await asyncio.sleep(self.rate_limit_seconds)\n",
        "            texto = await page.inner_text(\"body\")\n",
        "            await browser.close()\n",
        "            return {\"texto\": texto, \"metadata\": self.downloader.registrar_fonte(url)}\n",
        "\n",
        "\n",
        "class ChatIndexPersistente:\n",
        "    def __init__(self, storage_path: Path) -> None:\n",
        "        self.storage_path = storage_path\n",
        "        self.storage_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def registrar(self, conversa: Dict[str, Any]) -> None:\n",
        "        with self.storage_path.open(\"a\", encoding=\"utf-8\") as handle:\n",
        "            handle.write(json.dumps(conversa, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    def carregar(self) -> List[Dict[str, Any]]:\n",
        "        if not self.storage_path.exists():\n",
        "            return []\n",
        "        return [\n",
        "            json.loads(linha)\n",
        "            for linha in self.storage_path.read_text(encoding=\"utf-8\").splitlines()\n",
        "            if linha.strip()\n",
        "        ]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Exemplo: scraping simples de uma pÃ¡gina\n",
        "sistema = SistemaJuridicoUnificado(config)\n",
        "texto_coletado = asyncio.run(sistema.scraper.coletar_texto(\"https://www.gov.br/\"))\n",
        "print(texto_coletado[\"metadata\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SistemaAuditoriaUnificado:\n",
        "    def __init__(self, audit_dir: Path):\n",
        "        self.audit_dir = audit_dir\n",
        "        self.audit_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.log_central: List[Dict[str, Any]] = []\n",
        "        self.hash_registry: Dict[str, Dict[str, str]] = {}\n",
        "\n",
        "    def registrar_evento(self, categoria: str, evento: Dict[str, Any]) -> str:\n",
        "        evento_id = f\"evt_{hashlib.md5(str(evento).encode()).hexdigest()[:10]}\"\n",
        "        evento[\"timestamp\"] = evento.get(\"timestamp\", datetime.now().isoformat())\n",
        "\n",
        "        if self.log_central:\n",
        "            evento[\"hash_anterior\"] = self.hash_registry[self.log_central[-1][\"evento_id\"]][\"hash\"]\n",
        "\n",
        "        hash_atual = hashlib.md5(json.dumps(evento, sort_keys=True).encode()).hexdigest()\n",
        "        self.hash_registry[evento_id] = {\"hash\": hash_atual, \"timestamp\": evento[\"timestamp\"]}\n",
        "\n",
        "        registro = {**evento, \"evento_id\": evento_id, \"hash_atual\": hash_atual}\n",
        "        self.log_central.append(registro)\n",
        "        self._persistir_log(categoria, registro)\n",
        "        return evento_id\n",
        "\n",
        "    def _persistir_log(self, categoria: str, registro: Dict[str, Any]) -> None:\n",
        "        destino = self.audit_dir / f\"{categoria}.jsonl\"\n",
        "        with destino.open(\"a\", encoding=\"utf-8\") as handle:\n",
        "            handle.write(json.dumps(registro, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "class SistemaJuridicoUnificado:\n",
        "    def __init__(self, config: ConfigSistema):\n",
        "        self.config = config\n",
        "        self.auditoria = SistemaAuditoriaUnificado(config.audit_dir)\n",
        "        self.extrator = SistemaExtracaoDocling()\n",
        "        self.pageindex = PageIndexClient(config.pageindex_api_url, config.pageindex_api_key)\n",
        "        self.downloader = SistemaDownload(config.versao_fonte)\n",
        "        self.scraper = SistemaScrapingJuridico(self.downloader)\n",
        "        self.chatindex = ChatIndexPersistente(config.chatindex_dir / \"historico.jsonl\")\n",
        "\n",
        "    def _chunking_semantico(self, texto: str, tamanho: int = 800) -> List[str]:\n",
        "        return [texto[i:i + tamanho] for i in range(0, len(texto), tamanho)]\n",
        "\n",
        "    def _extrair_versoes_fontes(self, metadados_fontes: List[Dict[str, Any]]) -> List[str]:\n",
        "    def _gerar_resposta_llm(self, consulta: str, fontes: List[str], metadados_fontes: List[Dict[str, Any]]) -> str:\n",
        "        if not self.config.groq_api_key or self.config.groq_api_key == \"SUA_CHAVE_AQUI\":\n",
        "            return \"\"\n",
        "        analise_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"VocÃª Ã© um analista jurÃ­dico. Extraia pontos-chave e contexto necessÃ¡rio.\"),\n",
        "            (\"human\", \"Consulta: {consulta}\\nFontes: {fontes}\")\n",
        "        ])\n",
        "        resposta_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"VocÃª Ã© um assistente jurÃ­dico. Responda com base nos pontos-chave e fontes.\"),\n",
        "            (\"human\", \"Consulta: {consulta}\\nPontos-chave: {pontos}\\nMetadados: {metadados}\")\n",
        "        ])\n",
        "        llm_analise = ChatGroq(\n",
        "            groq_api_key=self.config.groq_api_key,\n",
        "            model=self.config.groq_model_analise,\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        llm_resposta = ChatGroq(\n",
        "            groq_api_key=self.config.groq_api_key,\n",
        "            model=self.config.groq_model_resposta,\n",
        "            temperature=0.3,\n",
        "        )\n",
        "        pontos = (analise_prompt | llm_analise | StrOutputParser()).invoke({\n",
        "            \"consulta\": consulta,\n",
        "            \"fontes\": \", \".join(fontes),\n",
        "        })\n",
        "        return (resposta_prompt | llm_resposta | StrOutputParser()).invoke({\n",
        "            \"consulta\": consulta,\n",
        "            \"pontos\": pontos,\n",
        "            \"metadados\": metadados_fontes,\n",
        "        })\n",
        "\n",
        "        return sorted({\n",
        "            item.get(\"versao_fonte\")\n",
        "            for item in metadados_fontes\n",
        "            if item.get(\"versao_fonte\")\n",
        "        })\n",
        "\n",
        "    def processar_documento(self, documento_path: Path) -> Dict[str, Any]:\n",
        "        self.auditoria.registrar_evento(\"processamento\", {\n",
        "            \"tipo\": \"inicio_processamento\",\n",
        "            \"documento\": documento_path.name,\n",
        "        })\n",
        "        extracao = self.extrator.extrair(documento_path)\n",
        "        chunks = self._chunking_semantico(extracao[\"texto_completo\"])\n",
        "        indexacao = self.pageindex.indexar(extracao[\"documento_id\"], chunks)\n",
        "        arvore = {\"documento_id\": documento_path.stem, \"estrutura_arvore\": {\"raiz\": {\"titulo\": documento_path.stem}}}\n",
        "\n",
        "        self.auditoria.registrar_evento(\"processamento\", {\n",
        "            \"tipo\": \"fim_processamento\",\n",
        "            \"documento\": documento_path.name,\n",
        "            \"chunks_gerados\": len(chunks),\n",
        "        })\n",
        "        return {\"extracao\": extracao, \"chunks\": chunks, \"arvore\": arvore, \"indexacao\": indexacao}\n",
        "\n",
        "    def responder_consulta(self, consulta: str, contexto: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        metadados_fontes = contexto.get(\"metadados_fontes\", [])\n",
        "        versoes_fontes = self._extrair_versoes_fontes(metadados_fontes)\n",
        "        self.auditoria.registrar_evento(\"consultas\", {\n",
        "            \"tipo\": \"consulta\",\n",
        "            \"consulta\": consulta,\n",
        "            \"versao_fonte\": versoes_fontes,\n",
        "        })\n",
        "        resposta_texto = self._gerar_resposta_llm(consulta, contexto.get(\"fontes\", []), metadados_fontes)\n",
        "        resposta = {\n",
        "            \"consulta\": consulta,\n",
        "            \"resposta\": resposta_texto or f\"Resumo jurÃ­dico para: {consulta}\",\n",
        "            \"fontes\": contexto.get(\"fontes\", []),\n",
        "            \"metadados_fontes\": metadados_fontes,\n",
        "            \"versao_fonte\": versoes_fontes,\n",
        "        }\n",
        "        self.auditoria.registrar_evento(\"consultas\", {\n",
        "            \"tipo\": \"resposta\",\n",
        "            \"consulta\": consulta,\n",
        "            \"hash_resposta\": hashlib.md5(json.dumps(resposta, ensure_ascii=False).encode()).hexdigest(),\n",
        "            \"versao_fonte\": versoes_fontes,\n",
        "        })\n",
        "        return resposta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Camada 2: Busca hÃ­brida e ranking\n",
        "\n",
        "A classe `SistemaBuscaHibrida` combina sinais do PageIndex e de embeddings vetoriais, com pesos configurÃ¡veis e explicabilidade do ranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SistemaBuscaHibrida:\n",
        "    def __init__(self, peso_pageindex: float = 0.6, peso_vetorial: float = 0.4):\n",
        "        self.peso_pageindex = peso_pageindex\n",
        "        self.peso_vetorial = peso_vetorial\n",
        "\n",
        "    def _normalizar(self, resultados: list[dict[str, float]]) -> dict[str, float]:\n",
        "        if not resultados:\n",
        "            return {}\n",
        "        max_score = max(item.get(\"score\", 0.0) for item in resultados) or 1.0\n",
        "        return {item[\"id\"]: item.get(\"score\", 0.0) / max_score for item in resultados}\n",
        "\n",
        "    def rankear(self, resultados_pageindex: list[dict[str, float]], resultados_vetoriais: list[dict[str, float]]):\n",
        "        pageindex_norm = self._normalizar(resultados_pageindex)\n",
        "        vetorial_norm = self._normalizar(resultados_vetoriais)\n",
        "        ids = set(pageindex_norm) | set(vetorial_norm)\n",
        "        combinados = []\n",
        "\n",
        "        for doc_id in ids:\n",
        "            score_pageindex = pageindex_norm.get(doc_id, 0.0)\n",
        "            score_vetorial = vetorial_norm.get(doc_id, 0.0)\n",
        "            score_final = (\n",
        "                self.peso_pageindex * score_pageindex +\n",
        "                self.peso_vetorial * score_vetorial\n",
        "            )\n",
        "            combinados.append({\n",
        "                \"id\": doc_id,\n",
        "                \"score_final\": score_final,\n",
        "                \"score_pageindex\": score_pageindex,\n",
        "                \"score_vetorial\": score_vetorial,\n",
        "            })\n",
        "\n",
        "        combinados.sort(key=lambda item: item[\"score_final\"], reverse=True)\n",
        "        return {\n",
        "            \"resultados\": combinados,\n",
        "            \"explicabilidade\": {\n",
        "                \"criterios\": (\n",
        "                    \"Score final = peso_pageindex * score_pageindex_normalizado + peso_vetorial * score_vetorial_normalizado\"\n",
        "                ),\n",
        "                \"pesos\": {\n",
        "                    \"peso_pageindex\": self.peso_pageindex,\n",
        "                    \"peso_vetorial\": self.peso_vetorial,\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "busca_hibrida = SistemaBuscaHibrida(peso_pageindex=0.7, peso_vetorial=0.3)\n",
        "rankeamento = busca_hibrida.rankear(\n",
        "    resultados_pageindex=[{\"id\": \"doc_1\", \"score\": 0.9}, {\"id\": \"doc_2\", \"score\": 0.7}],\n",
        "    resultados_vetoriais=[{\"id\": \"doc_2\", \"score\": 0.95}, {\"id\": \"doc_3\", \"score\": 0.6}],\n",
        ")\n",
        "rankeamento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat JurÃ­dico Robusto\n",
        "\n",
        "Este fluxo simula um chat jurÃ­dico que:\n",
        "- MantÃ©m histÃ³rico de contexto\n",
        "- Registra auditoria de cada interaÃ§Ã£o\n",
        "- Retorna respostas com fontes rastreÃ¡veis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ChatJuridico:\n",
        "    def __init__(self, sistema: SistemaJuridicoUnificado):\n",
        "        self.sistema = sistema\n",
        "        self.historico: List[Dict[str, Any]] = []\n",
        "\n",
        "    def enviar(self, consulta: str, fontes: Optional[List[str]] = None) -> Dict[str, Any]:\n",
        "        fontes_list = fontes or []\n",
        "        metadados_fontes = [\n",
        "            self.sistema.downloader.registrar_fonte(fonte)\n",
        "            for fonte in fontes_list\n",
        "        ]\n",
        "        contexto = {\"fontes\": fontes_list, \"metadados_fontes\": metadados_fontes}\n",
        "        resposta = self.sistema.responder_consulta(consulta, contexto)\n",
        "        self.historico.append({\"consulta\": consulta, \"resposta\": resposta})\n",
        "        self.sistema.chatindex.registrar({\"consulta\": consulta, \"resposta\": resposta})\n",
        "        return resposta\n",
        "\n",
        "\n",
        "sistema = SistemaJuridicoUnificado(config)\n",
        "chat = ChatJuridico(sistema)\n",
        "\n",
        "chat.enviar(\"Quais sÃ£o os requisitos para tutela de urgÃªncia?\", [\"STJ\", \"Planalto\"])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ConversaÃ§Ã£o de exemplo com o chat jurÃ­dico\n",
        "sistema = SistemaJuridicoUnificado(config)\n",
        "chat = ChatJuridico(sistema)\n",
        "\n",
        "chat.enviar(\"Quais sÃ£o os requisitos para tutela de urgÃªncia?\", [\"STJ\", \"Planalto\"])\n",
        "chat.enviar(\"Qual a base legal da responsabilidade civil objetiva?\", [\"STF\", \"Planalto\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AvaliaÃ§Ã£o e testes de regressÃ£o\n",
        "\n",
        "Exemplos simples de mÃ©tricas para validar a qualidade do ranking e detectar regressÃµes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def recall_at_k(resultados: List[str], relevantes: List[str], k: int = 5) -> float:\n",
        "    resultados_k = set(resultados[:k])\n",
        "    relevantes_set = set(relevantes)\n",
        "    if not relevantes_set:\n",
        "        return 0.0\n",
        "    return len(resultados_k & relevantes_set) / len(relevantes_set)\n",
        "\n",
        "\n",
        "def avaliacao_regressao(resultados: List[str], relevantes: List[str], limite: float = 0.6) -> Dict[str, Any]:\n",
        "    recall = recall_at_k(resultados, relevantes, k=5)\n",
        "    aprovado = recall >= limite\n",
        "    return {\"recall@5\": recall, \"limite\": limite, \"aprovado\": aprovado}\n",
        "\n",
        "\n",
        "avaliacao_regressao([\"doc_1\", \"doc_2\", \"doc_3\"], [\"doc_2\", \"doc_4\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PrÃ³ximos passos\n",
        "\n",
        "1. Substituir os placeholders por integraÃ§Ãµes reais (Docling, PageIndex API).\n",
        "2. Implementar scraping com Playwright e rate limiting.\n",
        "3. Enriquecer o ChatIndex com histÃ³rico persistente.\n",
        "4. Adicionar testes de regressÃ£o e avaliaÃ§Ã£o de qualidade.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}